from PIL import ImageDraw
import numpy as np
import matplotlib.pyplot as plt
import os
from skimage import io
join = os.path.join
from tqdm import tqdm
import torch
from torch.utils.data import Dataset, DataLoader
import monai
from segment_anything import SamPredictor, sam_model_registry
from segment_anything.utils.transforms import ResizeLongestSide
from utils.SurfaceDice import compute_dice_coefficient
import cv2
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, jaccard_score
# set seeds
torch.manual_seed(2023)
np.random.seed(2023)
from skimage import io
from  utils_metrics import *
from skimage import transform, io, segmentation
from segment.yolox import YOLOX
import random
import warnings

def get_cropped_model(img_path, output_path_seg, output_path_crop):
    # æ°¸ä¹…æ€§åœ°å¿½ç•¥æŒ‡å®šç±»å‹çš„è­¦å‘Š
    warnings.filterwarnings("ignore", category=UserWarning)
    #########################################################################################################
    ts_img_path = img_path
    # output_path = output_path
    model_type = 'vit_b'
    checkpoint = './pretrained_model/tonguesam.pth'
    device = 'cuda:0' 
    path_out_crop = output_path_crop
    path_out_seg = output_path_seg
    segment=YOLOX()
    ##############################################################################################################
    
    if not os.path.exists(path_out_seg):
        os.makedirs(path_out_seg)
    if not os.path.exists(path_out_crop):
        os.makedirs(path_out_crop)
    # if not os.path.exists(output_path):
    #     os.makedirs(output_path)

    sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to(device)
    sam_model.eval()

    for f in os.listdir(ts_img_path):   
        with torch.no_grad():             
            image_data = io.imread(join(ts_img_path, f))
            ori_image = image_data # ç”¨æœ€åˆçš„å›¾åƒè£å‰ª
        
            if image_data.shape[-1] > 3 and len(image_data.shape) == 3:
                image_data = image_data[:, :, :3]
            if len(image_data.shape) == 2:
                image_data = np.repeat(image_data[:, :, None], 3, axis=-1)
            
            lower_bound, upper_bound = np.percentile(image_data, 0.5), np.percentile(image_data, 99.5)
            image_data_pre = np.clip(image_data, lower_bound, upper_bound)
            image_data_pre = (image_data_pre - np.min(image_data_pre)) / (np.max(image_data_pre) - np.min(image_data_pre)) * 255.0
            image_data_pre[image_data == 0] = 0
            image_data_pre = transform.resize(image_data_pre, (400, 400), order=3, preserve_range=True, mode='constant', anti_aliasing=True)
            image_data_pre = np.uint8(image_data_pre)
            
            H, W, _ = image_data_pre.shape
            sam_transform = ResizeLongestSide(sam_model.image_encoder.img_size)
            resize_img = sam_transform.apply_image(image_data_pre)
            resize_img_tensor = torch.as_tensor(resize_img.transpose(2, 0, 1)).to(device)
            input_image = sam_model.preprocess(resize_img_tensor[None, :, :, :])        
            ts_img_embedding = sam_model.image_encoder(input_image)      

            img = image_data_pre
            boxes = segment.get_prompt(img)
                    
            if boxes is not None:
                sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)                   
                box = sam_trans.apply_boxes(boxes, (400,400))                                                
                box_torch = torch.as_tensor(box, dtype=torch.float, device=device)            
            else:            
                box_torch = None                
            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(
                points=None,
                boxes=box_torch,
                masks=None,
            )
            
            # ä½¿ç”¨Mask_Decoderç”Ÿæˆåˆ†å‰²ç»“æœ
            medsam_seg_prob, _ = sam_model.mask_decoder(
                image_embeddings=ts_img_embedding.to(device),
                image_pe=sam_model.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=False,
            )                        
            medsam_seg_prob =medsam_seg_prob.cpu().detach().numpy().squeeze()        
            medsam_seg = (medsam_seg_prob > 0.5).astype(np.uint8)
            
            medsam_seg=cv2.resize(medsam_seg,(400,400))       
            
            
            pred = cv2.Canny(cv2.resize((medsam_seg != 0).astype(np.uint8) * 255, (400, 400)), 100, 200)

            # è·å–åˆ†å‰²è£å‰ªåçš„å›¾
            orin_img = Image.fromarray(img)
            orin_img = orin_img.convert("RGBA")
            orin_img_np = np.array(orin_img)

            # å°†åŸå›¾è½¬æ¢ä¸ºRGBæ¨¡å¼
            orin_img_np = cv2.cvtColor(orin_img_np, cv2.COLOR_RGBA2RGB)

            # è·å–pred[i, j] != 0çš„æ‰€æœ‰ç‚¹
            y, x = np.nonzero(pred)

            # åˆ›å»ºä¸€ä¸ªå’ŒåŸå›¾ä¸€æ ·å¤§å°çš„å…¨é€æ˜çš„å›¾åƒ
            mask = np.zeros_like(orin_img_np)

            # åˆ›å»ºä¸€ä¸ªå’ŒåŸå›¾ä¸€æ ·å¤§å°çš„å…¨é»‘çš„å›¾åƒï¼Œç”¨äºå­˜å‚¨æ©ç 
            mask_img = np.zeros((orin_img_np.shape[0], orin_img_np.shape[1]), dtype=np.uint8)

            # ä½¿ç”¨è¿™äº›ç‚¹åˆ›å»ºä¸€ä¸ªæ©ç ï¼Œè¯¥æ©ç åœ¨å°é—­å›¾å½¢å†…éƒ¨çš„ç‚¹å¤„ä¸ºTrueï¼Œå…¶ä»–åœ°æ–¹ä¸ºFalse
            cv2.fillPoly(mask_img, [np.column_stack((x, y))], 1)

            # å°†æ©ç è½¬æ¢ä¸ºå’ŒåŸå›¾ä¸€æ ·çš„å½¢çŠ¶
            mask_img = np.stack([mask_img]*3, axis=-1)

            # ä½¿ç”¨è¿™ä¸ªæ©ç å’ŒåŸå›¾è¿›è¡ŒæŒ‰ä½ä¸æ“ä½œï¼Œå¾—åˆ°åªæœ‰å°é—­å›¾å½¢æœ‰é¢œè‰²çš„å›¾åƒ
            mask = np.where(mask_img, orin_img_np, 0)

            # å°†maskè½¬æ¢ä¸ºPILå›¾åƒï¼Œå¹¶è½¬æ¢ä¸ºRGBAæ¨¡å¼
            mask_pil = Image.fromarray(mask).convert('RGB')
            # mask_pil = mask_pil.convert('RGB')
            #ä¿å­˜åˆ° path_out_segè·¯å¾„
            mask_pil.save(os.path.join(path_out_seg, f'{os.path.splitext(f)[0]}.jpg'))


            image1 = Image.fromarray(medsam_seg)
            image2 = Image.fromarray(img)

            image1 = image1.resize(image2.size).convert("RGBA")
            image2 = image2.convert("RGB")
            data1 = image1.getdata()

            new_image = Image.new("RGBA", image2.size)
            new_data = [(0, 0, 128, 96) if pixel1[0] != 0 else (0, 0, 0, 0) for pixel1 in data1]

            new_image.putdata(new_data)
            if boxes is not None:              
                # draw = ImageDraw.Draw(image2)
                # draw.rectangle([boxes[0],boxes[1],boxes[2],boxes[3]],fill=None, outline=(0, 255, 0), width=1)  # ç”¨çº¢è‰²ç»˜åˆ¶æ–¹æ¡†çš„è¾¹æ¡†ï¼Œçº¿å®½ä¸º2
                # è·å–çŸ©å½¢è£å‰ªåçš„å›¾
                cropped = image2.crop((boxes[0],boxes[1],boxes[2],boxes[3]))
                # cropped = cropped.convert("RGB")
                cropped.save(os.path.join(path_out_crop, f'{os.path.splitext(f)[0]}.jpg'))
            print("Finish processing {} ğŸš€".format(f))

if __name__ == '__main__':
    get_cropped_model(img_path='./data/from_web/orin', output_path_crop='./data/from_web/crop', output_path_seg='./data/from_web/seg')       